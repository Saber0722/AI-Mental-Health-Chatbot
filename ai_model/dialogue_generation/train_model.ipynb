{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016f530b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 14:33:45.215155: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-02 14:33:48.635278: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-02 14:33:48.640962: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-02 14:33:49.279433: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-02 14:33:50.551057: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-02 14:33:58.362249: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/mnt/d/github/AI_Mental_Health_ChatBot/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import tensorflow as tf\n",
    "from transformers import TFT5ForConditionalGeneration, T5Tokenizer\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8439dbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 76673\n",
      "Validation samples: 12030\n"
     ]
    }
   ],
   "source": [
    "# loading the dataset\n",
    "dataset = load_dataset(\"empathetic_dialogues\", cache_dir=\"./cache\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"validation\"]\n",
    "print(\"Training samples:\", len(train_dataset))\n",
    "print(\"Validation samples:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b2b9872",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# loading the T5 model and tokenizer\n",
    "model_name = \"google-t5/t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = TFT5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35a36d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(examples):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    \n",
    "    # Group utterances by conversation ID\n",
    "    conv_id_to_dialogue = {}\n",
    "    for example in zip(examples[\"conv_id\"], examples[\"context\"], examples[\"utterance\"], examples[\"speaker_idx\"]):\n",
    "        conv_id, emotion, utterance, speaker_idx = example\n",
    "        if not isinstance(utterance, str) or not utterance.strip():  # Skip invalid utterances\n",
    "            continue\n",
    "        if conv_id not in conv_id_to_dialogue:\n",
    "            conv_id_to_dialogue[conv_id] = {\"emotion\": emotion, \"utterances\": [], \"speaker_indices\": []}\n",
    "        conv_id_to_dialogue[conv_id][\"utterances\"].append(utterance)\n",
    "        conv_id_to_dialogue[conv_id][\"speaker_indices\"].append(speaker_idx)\n",
    "    \n",
    "    # Create input-target pairs\n",
    "    for conv_id, data in conv_id_to_dialogue.items():\n",
    "        emotion = data[\"emotion\"]\n",
    "        utterances = data[\"utterances\"]\n",
    "        speaker_indices = data[\"speaker_indices\"]\n",
    "        for i in range(len(utterances)):\n",
    "            if speaker_indices[i] == 1:  # Responder's turn\n",
    "                context = \" \".join(utterances[max(0, i-2):i]) if i > 0 else \"\"\n",
    "                input_text = f\"emotion: {emotion} context: {context}\"\n",
    "                inputs.append(input_text)\n",
    "                targets.append(utterances[i])\n",
    "    \n",
    "    if not inputs or not targets:  # Check for empty lists\n",
    "        raise ValueError(\"No valid input-target pairs found in the dataset\")\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"np\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        model_targets = tokenizer(\n",
    "            targets,\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"np\"\n",
    "        )\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": model_inputs[\"input_ids\"],\n",
    "        \"attention_mask\": model_inputs[\"attention_mask\"],\n",
    "        \"labels\": model_targets[\"input_ids\"]\n",
    "    }\n",
    "\n",
    "train_encodings = preprocess_data(train_dataset)\n",
    "val_encodings = preprocess_data(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fda5ea63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tf_dataset(encodings, batch_size=4):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices({\n",
    "        \"input_ids\": tf.cast(encodings[\"input_ids\"], tf.int32),\n",
    "        \"attention_mask\": tf.cast(encodings[\"attention_mask\"], tf.int32),\n",
    "        \"labels\": tf.cast(encodings[\"labels\"], tf.int32)\n",
    "    })\n",
    "    return dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_tf_dataset = convert_to_tf_dataset(train_encodings)\n",
    "val_tf_dataset = convert_to_tf_dataset(val_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49b24ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "model.compile(optimizer=optimizer, loss= tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e31e961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "646/646 [==============================] - 320s 290ms/step - loss: 0.9818 - accuracy: 0.8673 - val_loss: 0.3396 - val_accuracy: 0.9364\n",
      "Epoch 2/3\n",
      "646/646 [==============================] - 150s 233ms/step - loss: 0.4142 - accuracy: 0.9222 - val_loss: 0.2932 - val_accuracy: 0.9410\n",
      "Epoch 3/3\n",
      "646/646 [==============================] - 153s 237ms/step - loss: 0.3760 - accuracy: 0.9260 - val_loss: 0.2773 - val_accuracy: 0.9422\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_tf_dataset,\n",
    "    validation_data=val_tf_dataset,\n",
    "    epochs=3,\n",
    "    verbose=1\n",
    ")\n",
    "model.save_pretrained(\"ai_model/fine_tuned_t5_empathetic_base\")\n",
    "tokenizer.save_pretrained(\"ai_model/fine_tuned_t5_empathetic_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "098b4b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: emotion: terrified context: \n",
      "Predicted Response: I'm going to be going to the gym next weekend.\n",
      "True Response: N/A (Speaker turn)\n"
     ]
    }
   ],
   "source": [
    "# Test on a sample\n",
    "sample = val_dataset[0]\n",
    "conv_id = sample[\"conv_id\"]\n",
    "emotion = sample[\"context\"]\n",
    "context = sample[\"utterance\"] if sample[\"speaker_idx\"] == 0 else \"\"\n",
    "input_text = f\"emotion: {emotion} context: {context}\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"tf\", max_length=128, truncation=True, padding=True)\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_length=128)\n",
    "predicted_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Input:\", input_text)\n",
    "print(\"Predicted Response:\", predicted_response)\n",
    "print(\"True Response:\", sample[\"utterance\"] if sample[\"speaker_idx\"] == 1 else \"N/A (Speaker turn)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3202495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: emotion: joyful context: \n",
      "Predicted Response: I'm going to be going to college next weekend. I'm going to be going to college next weekend.\n",
      "True Response: That's wonderful. How long have you guys been dating?\n"
     ]
    }
   ],
   "source": [
    "# Find a responder turn in the validation set\n",
    "for sample in val_dataset:\n",
    "    if sample[\"speaker_idx\"] == 1:  # Responder turn\n",
    "        conv_id = sample[\"conv_id\"]\n",
    "        emotion = sample[\"context\"]\n",
    "        # Get prior context from the same conversation\n",
    "        context = \"\"\n",
    "        for ex in val_dataset:\n",
    "            if ex[\"conv_id\"] == conv_id and ex[\"speaker_idx\"] == 0 and ex[\"utterance_idx\"] < sample[\"utterance_idx\"]:\n",
    "                context = ex[\"utterance\"]\n",
    "                break\n",
    "        input_text = f\"emotion: {emotion} context: {context}\"\n",
    "        inputs = tokenizer(input_text, return_tensors=\"tf\", max_length=128, truncation=True, padding=True)\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=128,\n",
    "            min_length=5,\n",
    "            num_beams=4,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.9\n",
    "        )\n",
    "        predicted_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(\"Input:\", input_text)\n",
    "        print(\"Predicted Response:\", predicted_response)\n",
    "        print(\"True Response:\", sample[\"utterance\"])\n",
    "        break  # Stop after one sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7140f5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Input IDs: [[13868    10 26128  2625    10    27    22    51  1829 26128     5     1]]\n",
      "Input: emotion: joyful context: Iâ€™m feeling joyful.\n",
      "Predicted Response: Oh no_comma_ how come?\n",
      "True Response: That's wonderful. How long have you guys been dating?\n"
     ]
    }
   ],
   "source": [
    "# Find a responder turn with context\n",
    "for sample in val_dataset:\n",
    "    if sample[\"speaker_idx\"] == 1:  # Responder turn\n",
    "        conv_id = sample[\"conv_id\"]\n",
    "        emotion = sample[\"context\"]\n",
    "        # Get the most recent prior speaker utterance\n",
    "        context = \"\"\n",
    "        for ex in val_dataset:\n",
    "            if ex[\"conv_id\"] == conv_id and ex[\"speaker_idx\"] == 0 and ex[\"utterance_idx\"] < sample[\"utterance_idx\"]:\n",
    "                context = ex[\"utterance\"]  # Use the latest speaker utterance\n",
    "        if not context:  # Skip if no context found\n",
    "            context = f\"Iâ€™m feeling {emotion}.\"\n",
    "        input_text = f\"emotion: {emotion} context: {context}\"\n",
    "        inputs = tokenizer(input_text, return_tensors=\"tf\", max_length=128, truncation=True, padding=True)\n",
    "        print(\"Tokenized Input IDs:\", inputs[\"input_ids\"].numpy())  # Debug input\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=128,\n",
    "            min_length=5,\n",
    "            num_beams=4,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.9,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "        predicted_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(\"Input:\", input_text)\n",
    "        print(\"Predicted Response:\", predicted_response)\n",
    "        print(\"True Response:\", sample[\"utterance\"])\n",
    "        break  # Stop after one sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "273c8638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Input IDs: [[13868    10 26128  2625    10    27    22    51 11646     3     9  1095\n",
      "    798     5     1]]\n",
      "Input: emotion: joyful context: Iâ€™m celebrating a happy moment.\n",
      "Predicted Response: Iâ€™m happy to have a happy moment.\n",
      "True Response: That's wonderful. How long have you guys been dating?\n"
     ]
    }
   ],
   "source": [
    "# Evaluate a responder turn with valid context\n",
    "found_valid_sample = False\n",
    "for sample in val_dataset:\n",
    "    if sample[\"speaker_idx\"] == 1:  # Responder turn\n",
    "        conv_id = sample[\"conv_id\"]\n",
    "        emotion = sample[\"context\"]\n",
    "        utterance_idx = sample[\"utterance_idx\"]\n",
    "        # Get the most recent prior speaker utterance\n",
    "        context = \"\"\n",
    "        prior_utterances = [\n",
    "            ex[\"utterance\"]\n",
    "            for ex in val_dataset\n",
    "            if ex[\"conv_id\"] == conv_id\n",
    "            and ex[\"speaker_idx\"] == 0\n",
    "            and ex[\"utterance_idx\"] < utterance_idx\n",
    "        ]\n",
    "        if prior_utterances:\n",
    "            context = prior_utterances[-1]  # Use the latest speaker utterance\n",
    "        else:\n",
    "            context = f\"Iâ€™m celebrating a happy moment.\" if emotion in [\"joyful\", \"happy\"] else f\"Iâ€™m feeling {emotion}.\"\n",
    "        input_text = f\"emotion: {emotion} context: {context}\"\n",
    "        inputs = tokenizer(input_text, return_tensors=\"tf\", max_length=128, truncation=True, padding=True)\n",
    "        print(\"Tokenized Input IDs:\", inputs[\"input_ids\"].numpy())  # Debug input\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=128,\n",
    "            min_length=10,\n",
    "            num_beams=6,\n",
    "            do_sample=False,  # Prioritize coherence\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "        predicted_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(\"Input:\", input_text)\n",
    "        print(\"Predicted Response:\", predicted_response)\n",
    "        print(\"True Response:\", sample[\"utterance\"])\n",
    "        found_valid_sample = True\n",
    "        break\n",
    "\n",
    "if not found_valid_sample:\n",
    "    print(\"No valid responder turn with context found in the validation set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "784ccf1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Case:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at ai_model/fine_tuned_t5_empathetic.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: emotion: joyful context: I just got accepted into my dream college!\n",
      "Generated Response: I'm so glad I got a job!\n",
      "--------------------------------------------------\n",
      "\n",
      "Test Case:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at ai_model/fine_tuned_t5_empathetic.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: emotion: sad context: I lost my job today and feel really down.\n",
      "Generated Response: I haven't been able to work for a long time.\n",
      "--------------------------------------------------\n",
      "\n",
      "Test Case:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at ai_model/fine_tuned_t5_empathetic.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: emotion: terrified context: I heard strange noises in my house last night.\n",
      "Generated Response: I heard strange noises in my house last night.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFT5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "def generate_empathetic_response(emotion, context, model_path=\"ai_model/fine_tuned_t5_empathetic\"):\n",
    "    \"\"\"\n",
    "    Generate an empathetic response using the fine-tuned T5 model.\n",
    "    \n",
    "    Args:\n",
    "        emotion (str): The emotion label (e.g., \"joyful\", \"sad\", \"terrified\").\n",
    "        context (str): The conversational context (e.g., \"I just got a promotion!\").\n",
    "        model_path (str): Path to the saved model and tokenizer.\n",
    "    \n",
    "    Returns:\n",
    "        str: The generated empathetic response.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the tokenizer and model\n",
    "        tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "        model = TFT5ForConditionalGeneration.from_pretrained(model_path)\n",
    "        \n",
    "        # Format input to match training data\n",
    "        input_text = f\"emotion: {emotion} context: {context}\"\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(\n",
    "            input_text,\n",
    "            return_tensors=\"tf\",\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        # Generate response\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=128,\n",
    "            min_length=10,\n",
    "            num_beams=6,\n",
    "            do_sample=False,  # Deterministic for consistency\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "        \n",
    "        # Decode response\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Print input and output\n",
    "        print(\"Input:\", input_text)\n",
    "        print(\"Generated Response:\", response)\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Test cases\n",
    "    test_cases = [\n",
    "        {\"emotion\": \"joyful\", \"context\": \"I just got accepted into my dream college!\"},\n",
    "        {\"emotion\": \"sad\", \"context\": \"I lost my job today and feel really down.\"},\n",
    "        {\"emotion\": \"terrified\", \"context\": \"I heard strange noises in my house last night.\"}\n",
    "    ]\n",
    "    \n",
    "    for case in test_cases:\n",
    "        print(\"\\nTest Case:\")\n",
    "        generate_empathetic_response(case[\"emotion\"], case[\"context\"])\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9876259a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Case:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at ai_model/fine_tuned_t5_empathetic.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Input IDs: [[ 3806     3    15    51 27826  1773    10 13868    10 26128  2625    10\n",
      "     27   131   530  4307   139    82  2461  1900    55     1]]\n",
      "Input: generate empathetic response: emotion: joyful context: I just got accepted into my dream college!\n",
      "Generated Response: I've been accepted into my dream college. I'm going to have a dream college!\n",
      "--------------------------------------------------\n",
      "\n",
      "Test Case:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at ai_model/fine_tuned_t5_empathetic.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Input IDs: [[ 3806     3    15    51 27826  1773    10 13868    10  6819  2625    10\n",
      "     27  1513    82   613   469    11   473   310   323     5     1]]\n",
      "Input: generate empathetic response: emotion: sad context: I lost my job today and feel really down.\n",
      "Generated Response: I don't have a job. It's a good job. I'm sorry I didn't work for a long time.\n",
      "--------------------------------------------------\n",
      "\n",
      "Test Case:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at ai_model/fine_tuned_t5_empathetic.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Input IDs: [[ 3806     3    15    51 27826  1773    10 13868    10 31539  2625    10\n",
      "     27  1943  6765  4661     7    16    82   629   336   706     5     1]]\n",
      "Input: generate empathetic response: emotion: terrified context: I heard strange noises in my house last night.\n",
      "Generated Response: I heard strange noises in my house last night. I heard a lot of noises from my house.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFT5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "def generate_empathetic_response(emotion, context, model_path=\"ai_model/fine_tuned_t5_empathetic\"):\n",
    "    \"\"\"\n",
    "    Generate an empathetic response using the fine-tuned T5 model.\n",
    "    \n",
    "    Args:\n",
    "        emotion (str): The emotion label (e.g., \"joyful\", \"sad\", \"terrified\").\n",
    "        context (str): The conversational context (e.g., \"I just got a promotion!\").\n",
    "        model_path (str): Path to the saved model and tokenizer.\n",
    "    \n",
    "    Returns:\n",
    "        str: The generated empathetic response.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Clear GPU memory\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        # Load the tokenizer and model\n",
    "        tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "        model = TFT5ForConditionalGeneration.from_pretrained(model_path)\n",
    "        \n",
    "        # Format input with task prefix to match training objective\n",
    "        input_text = f\"generate empathetic response: emotion: {emotion} context: {context}\"\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(\n",
    "            input_text,\n",
    "            return_tensors=\"tf\",\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        # Debug: Print tokenized input\n",
    "        print(\"Tokenized Input IDs:\", inputs[\"input_ids\"].numpy())\n",
    "        \n",
    "        # Generate response\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=128,\n",
    "            min_length=10,\n",
    "            num_beams=8,  # Increase for coherence\n",
    "            do_sample=True,  # Enable sampling for creativity\n",
    "            top_k=50,\n",
    "            top_p=0.9,\n",
    "            temperature=0.7,  # Balance creativity and coherence\n",
    "            no_repeat_ngram_size=3  # Allow some repetition for naturalness\n",
    "        )\n",
    "        \n",
    "        # Decode response\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Print input and output\n",
    "        print(\"Input:\", input_text)\n",
    "        print(\"Generated Response:\", response)\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Example usage with your test cases\n",
    "if __name__ == \"__main__\":\n",
    "    test_cases = [\n",
    "        {\"emotion\": \"joyful\", \"context\": \"I just got accepted into my dream college!\"},\n",
    "        {\"emotion\": \"sad\", \"context\": \"I lost my job today and feel really down.\"},\n",
    "        {\"emotion\": \"terrified\", \"context\": \"I heard strange noises in my house last night.\"}\n",
    "    ]\n",
    "    \n",
    "    for case in test_cases:\n",
    "        print(\"\\nTest Case:\")\n",
    "        generate_empathetic_response(case[\"emotion\"], case[\"context\"])\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a84d52e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conv_id': 'hit:3_conv:6', 'utterance_idx': 1, 'context': 'terrified', 'prompt': 'Today_comma_as i was leaving for work in the morning_comma_i had a tire burst in the middle of a busy road. That scared the hell out of me!', 'speaker_idx': 6, 'utterance': 'Today_comma_as i was leaving for work in the morning_comma_i had a tire burst in the middle of a busy road. That scared the hell out of me!', 'selfeval': '4|5|5_5|5|5', 'tags': ''}\n"
     ]
    }
   ],
   "source": [
    "print (val_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cbc1282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_52033/1476067516.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  bleu = load_metric(\"bleu\")\n",
      "Downloading builder script: 6.06kB [00:00, 7.95MB/s]                   \n",
      "Downloading extra modules: 4.07kB [00:00, 10.8MB/s]                   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[32m/tmp/ipykernel_52033/1476067516.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      8\u001b[39m     emotion = example[\u001b[33m\"context\"\u001b[39m]\n\u001b[32m      9\u001b[39m     context = example[\u001b[33m\"utterance\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m example[\u001b[33m\"speaker_idx\"\u001b[39m] == \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\"\u001b[39m\n\u001b[32m     10\u001b[39m     input_text = \u001b[33mf\"emotion: {emotion} context: {context}\"\u001b[39m\n\u001b[32m     11\u001b[39m     inputs = tokenizer(input_text, return_tensors=\u001b[33m\"tf\"\u001b[39m, max_length=\u001b[32m128\u001b[39m, truncation=\u001b[38;5;28;01mTrue\u001b[39;00m, padding=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     outputs = model.generate(input_ids=inputs[\u001b[33m\"input_ids\"\u001b[39m], attention_mask=inputs[\u001b[33m\"attention_mask\"\u001b[39m], max_length=\u001b[32m128\u001b[39m)\n\u001b[32m     13\u001b[39m     pred_response = tokenizer.decode(outputs[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m example[\u001b[33m\"speaker_idx\"\u001b[39m] == \u001b[32m1\u001b[39m:\n\u001b[32m     15\u001b[39m         predictions.append(pred_response.split())\n",
      "\u001b[32m/mnt/d/github/AI_Mental_Health_ChatBot/venv/lib/python3.11/site-packages/transformers/generation/tf_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, seed, **kwargs)\u001b[39m\n\u001b[32m    901\u001b[39m                     \u001b[33mf\"num_return_sequences has to be 1, but is {generation_config.num_return_sequences} when doing\"\u001b[39m\n\u001b[32m    902\u001b[39m                     \u001b[33m\" greedy search.\"\u001b[39m\n\u001b[32m    903\u001b[39m                 )\n\u001b[32m    904\u001b[39m             \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m905\u001b[39m             return self.greedy_search(\n\u001b[32m    906\u001b[39m                 input_ids,\n\u001b[32m    907\u001b[39m                 max_length=generation_config.max_length,\n\u001b[32m    908\u001b[39m                 pad_token_id=generation_config.pad_token_id,\n",
      "\u001b[32m/mnt/d/github/AI_Mental_Health_ChatBot/venv/lib/python3.11/site-packages/transformers/generation/tf_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, input_ids, max_length, pad_token_id, eos_token_id, logits_processor, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, **model_kwargs)\u001b[39m\n\u001b[32m   1740\u001b[39m \n\u001b[32m   1741\u001b[39m         \u001b[38;5;66;03m# 2-to-n generation steps can then be run in autoregressive fashion\u001b[39;00m\n\u001b[32m   1742\u001b[39m         \u001b[38;5;66;03m# only in case 1st generation step does NOT yield EOS token though\u001b[39;00m\n\u001b[32m   1743\u001b[39m         maximum_iterations = max_length - cur_len\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m         generated, _, cur_len, _ = tf.while_loop(\n\u001b[32m   1745\u001b[39m             greedy_search_cond_fn,\n\u001b[32m   1746\u001b[39m             greedy_search_body_fn,\n\u001b[32m   1747\u001b[39m             (generated, finished_sequences, cur_len, model_kwargs),\n",
      "\u001b[32m/mnt/d/github/AI_Mental_Health_ChatBot/venv/lib/python3.11/site-packages/tensorflow/python/util/deprecation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    656\u001b[39m                   _call_location(), decorator_utils.get_qualified_name(func),\n\u001b[32m    657\u001b[39m                   func.__module__, arg_name, arg_value,\n\u001b[32m    658\u001b[39m                   \u001b[33m'in a future version'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m date \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[32m    659\u001b[39m                   ('after %s' % date), instructions)\n\u001b[32m--> \u001b[39m\u001b[32m660\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[32m/mnt/d/github/AI_Mental_Health_ChatBot/venv/lib/python3.11/site-packages/tensorflow/python/ops/while_loop.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, maximum_iterations, name)\u001b[39m\n\u001b[32m    237\u001b[39m   ...   print(sess.run(x_out).shape)\n\u001b[32m    238\u001b[39m   (\u001b[32m1000\u001b[39m, \u001b[32m100\u001b[39m)\n\u001b[32m    239\u001b[39m \n\u001b[32m    240\u001b[39m   \"\"\"\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m   return while_loop(\n\u001b[32m    242\u001b[39m       cond=cond,\n\u001b[32m    243\u001b[39m       body=body,\n\u001b[32m    244\u001b[39m       loop_vars=loop_vars,\n",
      "\u001b[32m/mnt/d/github/AI_Mental_Health_ChatBot/venv/lib/python3.11/site-packages/tensorflow/python/ops/while_loop.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[39m\n\u001b[32m    484\u001b[39m \n\u001b[32m    485\u001b[39m       loop_var_structure = nest.map_structure(type_spec.type_spec_from_value,\n\u001b[32m    486\u001b[39m                                               list(loop_vars))\n\u001b[32m    487\u001b[39m       \u001b[38;5;28;01mwhile\u001b[39;00m cond(*loop_vars):\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m         loop_vars = body(*loop_vars)\n\u001b[32m    489\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m try_to_pack \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(loop_vars, (list, tuple)):\n\u001b[32m    490\u001b[39m           packed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    491\u001b[39m           loop_vars = (loop_vars,)\n",
      "\u001b[32m/mnt/d/github/AI_Mental_Health_ChatBot/venv/lib/python3.11/site-packages/tensorflow/python/ops/while_loop.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(i, lv)\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m         body = \u001b[38;5;28;01mlambda\u001b[39;00m i, lv: (i + \u001b[32m1\u001b[39m, orig_body(*lv))\n",
      "\u001b[32m/mnt/d/github/AI_Mental_Health_ChatBot/venv/lib/python3.11/site-packages/transformers/generation/tf_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(generated, finished_sequences, cur_len, model_kwargs)\u001b[39m\n\u001b[32m   1694\u001b[39m \n\u001b[32m   1695\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m eos_token_id \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1696\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m pad_token_id \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1697\u001b[39m                     \u001b[38;5;28;01mraise\u001b[39;00m ValueError(\u001b[33m\"If `eos_token_id` is defined, make sure that `pad_token_id` is defined.\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1698\u001b[39m                 unfinished_seq = \u001b[32m1\u001b[39m - tf.cast(finished_sequences, tf.int32)\n\u001b[32m   1699\u001b[39m                 next_tokens = next_tokens * unfinished_seq + pad_token_id * (\u001b[32m1\u001b[39m - unfinished_seq)\n\u001b[32m   1700\u001b[39m                 next_token_is_eos = tf.math.reduce_any(\n\u001b[32m   1701\u001b[39m                     tf.equal(\n",
      "\u001b[32m/mnt/d/github/AI_Mental_Health_ChatBot/venv/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m       filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    153\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m       \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[32m/mnt/d/github/AI_Mental_Health_ChatBot/venv/lib/python3.11/site-packages/tensorflow/python/ops/math_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(y, x)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ops.name_scope(\u001b[38;5;28;01mNone\u001b[39;00m, op_name, [x, y]) \u001b[38;5;28;01mas\u001b[39;00m name:\n\u001b[32m   1510\u001b[39m       \u001b[38;5;66;03m# TODO(b/178860388): Figure out why binary_op_wrapper and\u001b[39;00m\n\u001b[32m   1511\u001b[39m       \u001b[38;5;66;03m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[39;00m\n\u001b[32m   1512\u001b[39m       y, x = maybe_promote_tensors(y, x, force_same_dtype=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1513\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m func(x, y, name=name)\n",
      "\u001b[32m/mnt/d/github/AI_Mental_Health_ChatBot/venv/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m       filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    153\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m       \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[32m/mnt/d/github/AI_Mental_Health_ChatBot/venv/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1257\u001b[39m \n\u001b[32m   1258\u001b[39m       \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[32m   1259\u001b[39m       \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1260\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(*args, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1261\u001b[39m       \u001b[38;5;28;01mexcept\u001b[39;00m (TypeError, ValueError):\n\u001b[32m   1262\u001b[39m         \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[32m   1263\u001b[39m         \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[32m   1264\u001b[39m         result = dispatch(op_dispatch_handler, args, kwargs)\n",
      "\u001b[32m/mnt/d/github/AI_Mental_Health_ChatBot/venv/lib/python3.11/site-packages/tensorflow/python/ops/math_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x, y, name)\u001b[39m\n\u001b[32m    545\u001b[39m @tf_export(\u001b[33m\"math.subtract\"\u001b[39m, \u001b[33m\"subtract\"\u001b[39m)\n\u001b[32m    546\u001b[39m @dispatch.register_binary_elementwise_api\n\u001b[32m    547\u001b[39m @dispatch.add_dispatch_support\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m subtract(x, y, name=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m gen_math_ops.sub(x, y, name)\n",
      "\u001b[32m/mnt/d/github/AI_Mental_Health_ChatBot/venv/lib/python3.11/site-packages/tensorflow/python/ops/weak_tensor_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    140\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m wrapper(*args, **kwargs):\n\u001b[32m    141\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m ops.is_auto_dtype_conversion_enabled():\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m op(*args, **kwargs)\n\u001b[32m    143\u001b[39m     bound_arguments = signature.bind(*args, **kwargs)\n\u001b[32m    144\u001b[39m     bound_arguments.apply_defaults()\n\u001b[32m    145\u001b[39m     bound_kwargs = bound_arguments.arguments\n",
      "\u001b[32m/mnt/d/github/AI_Mental_Health_ChatBot/venv/lib/python3.11/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x, y, name)\u001b[39m\n\u001b[32m  12231\u001b[39m         _ctx, \"Sub\", name, x, y)\n\u001b[32m  12232\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m  12233\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m  12234\u001b[39m       _ops.raise_from_not_ok_status(e, name)\n\u001b[32m> \u001b[39m\u001b[32m12235\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m _core._FallbackException:\n\u001b[32m  12236\u001b[39m       \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m  12237\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m  12238\u001b[39m       return sub_eager_fallback(\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "bleu = load_metric(\"bleu\")\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "for example in val_dataset.select(range(50)):\n",
    "    conv_id = example[\"conv_id\"]\n",
    "    emotion = example[\"context\"]\n",
    "    context = example[\"utterance\"] if example[\"speaker_idx\"] == 0 else \"\"\n",
    "    input_text = f\"emotion: {emotion} context: {context}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"tf\", max_length=128, truncation=True, padding=True)\n",
    "    outputs = model.generate(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_length=128)\n",
    "    pred_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if example[\"speaker_idx\"] == 1:\n",
    "        predictions.append(pred_response.split())\n",
    "        references.append([example[\"utterance\"].split()])\n",
    "\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "print(\"BLEU Score:\", results[\"bleu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9ea048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ccf863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8429f19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36574c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d22de8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 16:22:49.363347: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-02 16:22:52.230844: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-02 16:22:52.235678: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-02 16:22:52.836749: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-02 16:22:53.965243: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-02 16:23:03.751286: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d54e0e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 76673\n",
      "Validation samples: 12030\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"empathetic_dialogues\", cache_dir=\"./cache\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"validation\"]\n",
    "print(\"Training samples:\", len(train_dataset))\n",
    "print(\"Validation samples:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd82eb44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartEncoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartDecoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50264, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"sshleifer/distilbart-cnn-6-6\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Move model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b683c174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(examples, task_prefix=\"generate empathetic response: \"):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    \n",
    "    # Group utterances by conversation ID\n",
    "    conv_id_to_dialogue = {}\n",
    "    for example in zip(examples[\"conv_id\"], examples[\"context\"], examples[\"utterance\"], examples[\"speaker_idx\"]):\n",
    "        conv_id, emotion, utterance, speaker_idx = example\n",
    "        if not isinstance(utterance, str) or not utterance.strip() or len(utterance.split()) < 3:\n",
    "            continue\n",
    "        if conv_id not in conv_id_to_dialogue:\n",
    "            conv_id_to_dialogue[conv_id] = {\"emotion\": emotion, \"utterances\": [], \"speaker_indices\": []}\n",
    "        conv_id_to_dialogue[conv_id][\"utterances\"].append(utterance)\n",
    "        conv_id_to_dialogue[conv_id][\"speaker_indices\"].append(speaker_idx)\n",
    "    \n",
    "    # Create input-target pairs\n",
    "    for conv_id, data in conv_id_to_dialogue.items():\n",
    "        emotion = data[\"emotion\"]\n",
    "        utterances = data[\"utterances\"]\n",
    "        speaker_indices = data[\"speaker_indices\"]\n",
    "        for i in range(len(utterances)):\n",
    "            if speaker_indices[i] == 1:  # Responder turn\n",
    "                context = \" \".join(utterances[max(0, i-3):i]) if i > 0 else f\"Iâ€™m feeling {emotion}.\"\n",
    "                input_text = f\"{task_prefix}emotion: {emotion} context: {context}\"\n",
    "                target = utterances[i]\n",
    "                # Filter out targets too similar to context or too generic\n",
    "                similarity = SequenceMatcher(None, context.lower(), target.lower()).ratio()\n",
    "                if similarity < 0.7 and len(target.split()) >= 5 and target.lower() not in [\"thatâ€™s nice.\", \"okay.\", \"cool.\"]:\n",
    "                    inputs.append(input_text)\n",
    "                    targets.append(target)\n",
    "    \n",
    "    if not inputs or not targets:\n",
    "        raise ValueError(\"No valid input-target pairs found in the dataset\")\n",
    "    \n",
    "    # Tokenize\n",
    "    encodings = tokenizer(\n",
    "        inputs,\n",
    "        max_length=64,  # Reduced for GPU\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    target_encodings = tokenizer(\n",
    "        targets,\n",
    "        max_length=64,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": encodings[\"input_ids\"],\n",
    "        \"attention_mask\": encodings[\"attention_mask\"],\n",
    "        \"labels\": target_encodings[\"input_ids\"]\n",
    "    }\n",
    "\n",
    "train_encodings = preprocess_data(train_dataset)\n",
    "val_encodings = preprocess_data(val_dataset)\n",
    "\n",
    "# Create PyTorch dataset\n",
    "from torch.utils.data import Dataset\n",
    "class EmpatheticDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.encodings[\"input_ids\"][idx],\n",
    "            \"attention_mask\": self.encodings[\"attention_mask\"][idx],\n",
    "            \"labels\": self.encodings[\"labels\"][idx]\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "train_dataset = EmpatheticDataset(train_encodings)\n",
    "val_dataset = EmpatheticDataset(val_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "169ef626",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=8,\n",
    "    per_device_train_batch_size=2,  # Small batch size for GPU\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,  # Effective batch size = 8\n",
    "    fp16=True,  # Mixed precision\n",
    "    learning_rate=3e-5,\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e5a2d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2400' max='2400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2400/2400 13:26, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.766100</td>\n",
       "      <td>0.620654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.613600</td>\n",
       "      <td>0.605920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.477600</td>\n",
       "      <td>0.624932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.370000</td>\n",
       "      <td>0.668107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.290200</td>\n",
       "      <td>0.734318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.229200</td>\n",
       "      <td>0.777126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.187000</td>\n",
       "      <td>0.807861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.157500</td>\n",
       "      <td>0.824522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/github/AI_Mental_Health_ChatBot/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:3464: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('ai_model/fine_tuned_distilbart_empathetic/tokenizer_config.json',\n",
       " 'ai_model/fine_tuned_distilbart_empathetic/special_tokens_map.json',\n",
       " 'ai_model/fine_tuned_distilbart_empathetic/vocab.json',\n",
       " 'ai_model/fine_tuned_distilbart_empathetic/merges.txt',\n",
       " 'ai_model/fine_tuned_distilbart_empathetic/added_tokens.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "trainer.train()\n",
    "model.save_pretrained(\"ai_model/fine_tuned_distilbart_empathetic\")\n",
    "tokenizer.save_pretrained(\"ai_model/fine_tuned_distilbart_empathetic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64f24a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Case:\n",
      "Tokenized Input IDs: [[    0 20557   877  2841 11632 18667  1263    35 11926    35 32076  5377\n",
      "     35    38    95   300  3903    88   127  3366  1564   328     2]]\n",
      "Input: generate empathetic response: emotion: joyful context: I just got accepted into my dream college!\n",
      "Generated Response: Yea, it is. It's a lot of fun!\n",
      "--------------------------------------------------\n",
      "\n",
      "Test Case:\n",
      "Tokenized Input IDs: [[    0 20557   877  2841 11632 18667  1263    35 11926    35  5074  5377\n",
      "     35    38   685   127   633   452     8   619   269   159     4     2]]\n",
      "Input: generate empathetic response: emotion: sad context: I lost my job today and feel really down.\n",
      "Generated Response: I'm sorry to hear that. How long have you been without your job?\n",
      "--------------------------------------------------\n",
      "\n",
      "Test Case:\n",
      "Tokenized Input IDs: [[    0 20557   877  2841 11632 18667  1263    35 11926    35 19419  5377\n",
      "     35    38  1317  7782 27903    11   127   790    94   363     4     2]]\n",
      "Input: generate empathetic response: emotion: terrified context: I heard strange noises in my house last night.\n",
      "Generated Response: Oh no, what happened last night?\n",
      "--------------------------------------------------\n",
      "\n",
      "Test Case:\n",
      "Tokenized Input IDs: [[    0 20557   877  2841 11632 18667  1263    35 11926    35  2283  5377\n",
      "     35    38    17    27   119  1158    10    92   633   220   186   328\n",
      "      2]]\n",
      "Input: generate empathetic response: emotion: excited context: Iâ€™m starting a new job next week!\n",
      "Generated Response: I'm excited to hear that! What are you looking forward to doing next week?\n",
      "--------------------------------------------------\n",
      "\n",
      "Test Case:\n",
      "Tokenized Input IDs: [[    0 20557   877  2841 11632 18667  1263    35 11926    35  8164  5377\n",
      "     35  1308  3034  6050     8    38   685   127   695     4     2]]\n",
      "Input: generate empathetic response: emotion: frustrated context: My computer crashed and I lost my project.\n",
      "Generated Response: Oh no, why did you lose your project?\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def generate_empathetic_response(emotion, context, model_path=\"ai_model/fine_tuned_distilbart_empathetic\"):\n",
    "    \"\"\"\n",
    "    Generate an empathetic response using the fine-tuned DistilBART model, replacing '_comma_' with ','.\n",
    "    \n",
    "    Args:\n",
    "        emotion (str): The emotion label (e.g., \"joyful\", \"sad\", \"terrified\").\n",
    "        context (str): The conversational context (e.g., \"I just got a promotion!\").\n",
    "        model_path (str): Path to the saved model and tokenizer.\n",
    "    \n",
    "    Returns:\n",
    "        str: The generated empathetic response with '_comma_' replaced by ','.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load tokenizer and model\n",
    "        tokenizer = BartTokenizer.from_pretrained(model_path)\n",
    "        model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "        \n",
    "        # Format input\n",
    "        input_text = f\"generate empathetic response: emotion: {emotion} context: {context}\"\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(\n",
    "            input_text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=64,\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        ).to(device)\n",
    "        \n",
    "        # Debug: Print tokenized input\n",
    "        print(\"Tokenized Input IDs:\", inputs[\"input_ids\"].cpu().numpy())\n",
    "        \n",
    "        # Generate response\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=64,\n",
    "            min_length=10,\n",
    "            num_beams=8,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.9,\n",
    "            temperature=0.7,\n",
    "            no_repeat_ngram_size=3\n",
    "        )\n",
    "        \n",
    "        # Decode response\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Replace '_comma_' with ','\n",
    "        clean_response = response.replace(\"_comma_\", \",\")\n",
    "        \n",
    "        # Print input and output\n",
    "        print(\"Input:\", input_text)\n",
    "        print(\"Generated Response:\", clean_response)\n",
    "        \n",
    "        return clean_response\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Test cases\n",
    "if __name__ == \"__main__\":\n",
    "    test_cases = [\n",
    "        {\"emotion\": \"joyful\", \"context\": \"I just got accepted into my dream college!\"},\n",
    "        {\"emotion\": \"sad\", \"context\": \"I lost my job today and feel really down.\"},\n",
    "        {\"emotion\": \"terrified\", \"context\": \"I heard strange noises in my house last night.\"},\n",
    "        {\"emotion\": \"excited\", \"context\": \"Iâ€™m starting a new job next week!\"},\n",
    "        {\"emotion\": \"frustrated\", \"context\": \"My computer crashed and I lost my project.\"}\n",
    "    ]\n",
    "    for case in test_cases:\n",
    "        print(\"\\nTest Case:\")\n",
    "        generate_empathetic_response(case[\"emotion\"], case[\"context\"])\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00e9db4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Case:\n",
      "Tokenized Input IDs: [[    0 20557   877  2841 11632 18667  1263    35 11926    35 32076  5377\n",
      "     35    38    95   300  3903    88   127  3366  1564   328     2]]\n",
      "Input: generate empathetic response: emotion: joyful context: I just got accepted into my dream college!\n",
      "Generated Response: Yea, it is. It's a lot of fun!\n",
      "--------------------------------------------------\n",
      "\n",
      "Test Case:\n",
      "Tokenized Input IDs: [[    0 20557   877  2841 11632 18667  1263    35 11926    35  5074  5377\n",
      "     35    38   685   127   633   452     8   619   269   159     4     2]]\n",
      "Input: generate empathetic response: emotion: sad context: I lost my job today and feel really down.\n",
      "Generated Response: Oh no, why did you lose your job?\n",
      "--------------------------------------------------\n",
      "\n",
      "Test Case:\n",
      "Tokenized Input IDs: [[    0 20557   877  2841 11632 18667  1263    35 11926    35 19419  5377\n",
      "     35    38  1317  7782 27903    11   127   790    94   363     4     2]]\n",
      "Input: generate empathetic response: emotion: terrified context: I heard strange noises in my house last night.\n",
      "Generated Response: Oh no, what happened last night?\n",
      "--------------------------------------------------\n",
      "\n",
      "Test Case:\n",
      "Tokenized Input IDs: [[    0 20557   877  2841 11632 18667  1263    35 11926    35  2283  5377\n",
      "     35    38    17    27   119  1158    10    92   633   220   186   328\n",
      "      2]]\n",
      "Input: generate empathetic response: emotion: excited context: Iâ€™m starting a new job next week!\n",
      "Generated Response: Yea, I'm looking forward to it. It'll be a lot of fun!\n",
      "--------------------------------------------------\n",
      "\n",
      "Test Case:\n",
      "Tokenized Input IDs: [[    0 20557   877  2841 11632 18667  1263    35 11926    35  8164  5377\n",
      "     35  1308  3034  6050     8    38   685   127   695     4     2]]\n",
      "Input: generate empathetic response: emotion: frustrated context: My computer crashed and I lost my project.\n",
      "Generated Response: Oh no, why did you lose your project?\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def generate_empathetic_response(emotion, context, model_path=\"ai_model/fine_tuned_distilbart_empathetic\"):\n",
    "    \"\"\"\n",
    "    Generate an empathetic response using the fine-tuned DistilBART model, replacing '_comma_' with ','.\n",
    "    \n",
    "    Args:\n",
    "        emotion (str): The emotion label (e.g., \"joyful\", \"sad\", \"terrified\").\n",
    "        context (str): The conversational context (e.g., \"I just got a promotion!\").\n",
    "        model_path (str): Path to the saved model and tokenizer.\n",
    "    \n",
    "    Returns:\n",
    "        str: The generated empathetic response with '_comma_' replaced by ','.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load tokenizer and model\n",
    "        tokenizer = BartTokenizer.from_pretrained(model_path)\n",
    "        model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "        \n",
    "        # Format input\n",
    "        input_text = f\"generate empathetic response: emotion: {emotion} context: {context}\"\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(\n",
    "            input_text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=64,\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        ).to(device)\n",
    "        \n",
    "        # Debug: Print tokenized input\n",
    "        print(\"Tokenized Input IDs:\", inputs[\"input_ids\"].cpu().numpy())\n",
    "        \n",
    "        # Generate response\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=64,\n",
    "            min_length=10,\n",
    "            num_beams=8,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.8,\n",
    "            no_repeat_ngram_size=3\n",
    "        )\n",
    "        \n",
    "        # Decode response\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Replace '_comma_' with ','\n",
    "        clean_response = response.replace(\"_comma_\", \",\")\n",
    "        \n",
    "        # Print input and output\n",
    "        print(\"Input:\", input_text)\n",
    "        print(\"Generated Response:\", clean_response)\n",
    "        \n",
    "        return clean_response\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Test cases\n",
    "if __name__ == \"__main__\":\n",
    "    test_cases = [\n",
    "        {\"emotion\": \"joyful\", \"context\": \"I just got accepted into my dream college!\"},\n",
    "        {\"emotion\": \"sad\", \"context\": \"I lost my job today and feel really down.\"},\n",
    "        {\"emotion\": \"terrified\", \"context\": \"I heard strange noises in my house last night.\"},\n",
    "        {\"emotion\": \"excited\", \"context\": \"Iâ€™m starting a new job next week!\"},\n",
    "        {\"emotion\": \"frustrated\", \"context\": \"My computer crashed and I lost my project.\"}\n",
    "    ]\n",
    "    for case in test_cases:\n",
    "        print(\"\\nTest Case:\")\n",
    "        generate_empathetic_response(case[\"emotion\"], case[\"context\"])\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979b49f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test cases:\n",
      "\n",
      "Test Case:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca9370388814b7bb0146e35f8433a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51de67333c7b40e2b4aa5a5e4777fcea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/329M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c561d1e97b1477ea414187c604470df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/294 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee4f6a14daf4966b54c45889765cff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b52b9d9abde4d69921c9d67c556bdb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/329M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54cab3080cfe45a2ae4e1bc3461854a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4962ce6336e246a8a1e3b5aeb96beb2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9acf2a85c758416db5e61987a6990c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred Emotion: joyful\n",
      "Tokenized Input IDs: [[    0 20557   877  2841 11632 18667  1263    35 11926    35 32076  5377\n",
      "     35    38    95   300  3903    88   127  3366  1564   328     2]]\n",
      "User Prompt: I just got accepted into my dream college!\n",
      "Generated Response: Yea, it is. It's a lot of fun!\n",
      "--------------------------------------------------\n",
      "\n",
      "Test Case:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred Emotion: sad\n",
      "Tokenized Input IDs: [[    0 20557   877  2841 11632 18667  1263    35 11926    35  5074  5377\n",
      "     35    38   685   127   633   452     8   619   269   159     4     2]]\n",
      "User Prompt: I lost my job today and feel really down.\n",
      "Generated Response: Oh no, why did you lose your job?\n",
      "--------------------------------------------------\n",
      "\n",
      "Test Case:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred Emotion: terrified\n",
      "Tokenized Input IDs: [[    0 20557   877  2841 11632 18667  1263    35 11926    35 19419  5377\n",
      "     35    38  1317  7782 27903    11   127   790    94   363     4     2]]\n",
      "User Prompt: I heard strange noises in my house last night.\n",
      "Generated Response: Oh no, what happened last night?\n",
      "--------------------------------------------------\n",
      "\n",
      "Test Case:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred Emotion: joyful\n",
      "Tokenized Input IDs: [[    0 20557   877  2841 11632 18667  1263    35 11926    35 32076  5377\n",
      "     35    38    17    27   119  1158    10    92   633   220   186   328\n",
      "      2]]\n",
      "User Prompt: Iâ€™m starting a new job next week!\n",
      "Generated Response: That's awesome! What's going on next week?\n",
      "--------------------------------------------------\n",
      "\n",
      "Test Case:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred Emotion: sad\n",
      "Tokenized Input IDs: [[    0 20557   877  2841 11632 18667  1263    35 11926    35  5074  5377\n",
      "     35  1308  3034  6050     8    38   685   127   695     4     2]]\n",
      "User Prompt: My computer crashed and I lost my project.\n",
      "Generated Response: Oh no, why did you lose your project?\n",
      "--------------------------------------------------\n",
      "\n",
      "Starting interactive chatbot...\n",
      "Welcome to the Empathetic Chatbot! Type your message or 'quit' to exit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred Emotion: neutral\n",
      "Tokenized Input IDs: [[    0 20557   877  2841 11632 18667  1263    35 11926    35  7974  5377\n",
      "     35 12289     2]]\n",
      "User Prompt: Hi\n",
      "Generated Response: Oh no, what happened?\n",
      "Please enter a message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred Emotion: joyful\n",
      "Tokenized Input IDs: [[    0 20557   877  2841 11632 18667  1263    35 11926    35 32076  5377\n",
      "     35    38  1550   127   695 32376     2]]\n",
      "User Prompt: I finished my project!!!!\n",
      "Generated Response: That's awesome! What project was it?\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, pipeline\n",
    "\n",
    "def chat_with_bot(prompt, model_path=\"ai_model/fine_tuned_distilbart_empathetic\"):\n",
    "    \"\"\"\n",
    "    Generate an empathetic response from a single user prompt by inferring emotion.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): User input text (e.g., \"I just got accepted into my dream college!\").\n",
    "        model_path (str): Path to the fine-tuned DistilBART model.\n",
    "    \n",
    "    Returns:\n",
    "        str: Empathetic response with '_comma_' replaced by ','.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Clear GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Load emotion classifier\n",
    "        emotion_classifier = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=\"j-hartmann/emotion-english-distilroberta-base\",\n",
    "            top_k=None,\n",
    "            device=0 if torch.cuda.is_available() else -1\n",
    "        )\n",
    "        \n",
    "        # Infer emotion from prompt\n",
    "        emotion_scores = emotion_classifier(prompt)[0]\n",
    "        # Map to Empathetic Dialogues emotions\n",
    "        emotion_map = {\n",
    "            \"joy\": \"joyful\",\n",
    "            \"sadness\": \"sad\",\n",
    "            \"fear\": \"terrified\",\n",
    "            \"anger\": \"angry\",\n",
    "            \"surprise\": \"excited\",\n",
    "            \"disgust\": \"frustrated\",\n",
    "            \"neutral\": \"neutral\"\n",
    "        }\n",
    "        # Select highest-scoring emotion\n",
    "        top_emotion = max(emotion_scores, key=lambda x: x[\"score\"])[\"label\"]\n",
    "        emotion = emotion_map.get(top_emotion, \"neutral\")\n",
    "        \n",
    "        # Load DistilBART model and tokenizer\n",
    "        tokenizer = BartTokenizer.from_pretrained(model_path)\n",
    "        model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "        \n",
    "        # Format input\n",
    "        input_text = f\"generate empathetic response: emotion: {emotion} context: {prompt}\"\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(\n",
    "            input_text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=64,\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        ).to(device)\n",
    "        \n",
    "        # Debug: Print tokenized input and inferred emotion\n",
    "        print(\"Inferred Emotion:\", emotion)\n",
    "        print(\"Tokenized Input IDs:\", inputs[\"input_ids\"].cpu().numpy())\n",
    "        \n",
    "        # Generate response\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=64,\n",
    "            min_length=10,\n",
    "            num_beams=8,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.9,\n",
    "            temperature=0.8,  # Increased for diversity\n",
    "            no_repeat_ngram_size=3\n",
    "        )\n",
    "        \n",
    "        # Decode response\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Replace '_comma_' with ','\n",
    "        clean_response = response.replace(\"_comma_\", \",\")\n",
    "        \n",
    "        # Print input and output\n",
    "        print(\"User Prompt:\", prompt)\n",
    "        print(\"Generated Response:\", clean_response)\n",
    "        \n",
    "        return clean_response\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Interactive chatbot loop\n",
    "def run_chatbot(model_path=\"ai_model/fine_tuned_distilbart_empathetic\"):\n",
    "    \"\"\"\n",
    "    Run an interactive chatbot session.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the fine-tuned DistilBART model.\n",
    "    \"\"\"\n",
    "    print(\"Welcome to the Empathetic Chatbot! Type your message or 'quit' to exit.\")\n",
    "    conversation_log = []\n",
    "    \n",
    "    while True:\n",
    "        prompt = input(\"You: \").strip()\n",
    "        if prompt.lower() == \"quit\":\n",
    "            break\n",
    "        if not prompt:\n",
    "            print(\"Please enter a message.\")\n",
    "            continue\n",
    "        \n",
    "        response = chat_with_bot(prompt, model_path)\n",
    "        if response:\n",
    "            conversation_log.append({\"prompt\": prompt, \"response\": response})\n",
    "    \n",
    "    # Save conversation log\n",
    "    import json\n",
    "    with open(\"conversation_log.json\", \"w\") as f:\n",
    "        json.dump(conversation_log, f, indent=2)\n",
    "    print(\"Conversation saved to conversation_log.json\")\n",
    "    print(\"Goodbye!\")\n",
    "\n",
    "# Test cases\n",
    "if __name__ == \"__main__\":\n",
    "    test_prompts = [\n",
    "        \"I just got accepted into my dream college!\",\n",
    "        \"I lost my job today and feel really down.\",\n",
    "        \"I heard strange noises in my house last night.\",\n",
    "        \"Iâ€™m starting a new job next week!\",\n",
    "        \"My computer crashed and I lost my project.\"\n",
    "    ]\n",
    "    print(\"Running test cases:\")\n",
    "    for prompt in test_prompts:\n",
    "        print(\"\\nTest Case:\")\n",
    "        chat_with_bot(prompt)\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    # Start interactive chatbot\n",
    "    print(\"\\nStarting interactive chatbot...\")\n",
    "    run_chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdba1bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
